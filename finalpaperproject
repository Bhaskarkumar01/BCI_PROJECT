{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":14813774,"datasetId":9472879,"databundleVersionId":15670034},{"sourceType":"datasetVersion","sourceId":14787799,"datasetId":9453807,"databundleVersionId":15641611}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"print(\"hi\")","metadata":{"_uuid":"650c1761-0a4b-41be-b359-8935624e977c","_cell_guid":"90411b0f-08c8-49f1-a4db-dcdd1a0a80b6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2026-02-20T21:45:07.084142Z","iopub.execute_input":"2026-02-20T21:45:07.084376Z","iopub.status.idle":"2026-02-20T21:45:07.091674Z","shell.execute_reply.started":"2026-02-20T21:45:07.084354Z","shell.execute_reply":"2026-02-20T21:45:07.090865Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mkdir -p /kaggle/working/project/data\n!mkdir -p /kaggle/working/project/model\n!mkdir -p /kaggle/working/project/results","metadata":{"_uuid":"8df6dfb8-6fdd-4bc3-8cd5-1b167edfdfe1","_cell_guid":"e93a254d-f2cc-4f87-99ea-1ba75f96e8f8","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-02-20T21:45:17.345478Z","iopub.execute_input":"2026-02-20T21:45:17.345767Z","iopub.status.idle":"2026-02-20T21:45:17.680907Z","shell.execute_reply.started":"2026-02-20T21:45:17.345740Z","shell.execute_reply":"2026-02-20T21:45:17.679964Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!cp -r /kaggle/input/datasets/bhaskarkumar01/bciciv-2a /kaggle/working/project/data/BCICIV_2a_gdf","metadata":{"_uuid":"36c4ec08-5a24-4b7c-9155-82fd58057fe0","_cell_guid":"865a3806-2d21-4134-9b07-ea82d0d15ffb","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-02-20T21:45:18.198777Z","iopub.execute_input":"2026-02-20T21:45:18.199129Z","iopub.status.idle":"2026-02-20T21:45:23.818655Z","shell.execute_reply.started":"2026-02-20T21:45:18.199094Z","shell.execute_reply":"2026-02-20T21:45:23.817709Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /kaggle/working/project/data/utils.py\nimport torch\nimport numpy as np\nfrom scipy.signal import butter, lfilter\n\ndef accuracy(preds, labels):\n    _, pred = torch.max(preds, 1)\n    return (pred == labels).float().mean().item()\n\ndef bandpass_filter(data, low=8, high=30, fs=250, order=4):\n    nyq = 0.5 * fs\n    low /= nyq\n    high /= nyq\n    b, a = butter(order, [low, high], btype=\"band\")\n    return lfilter(b, a, data, axis=-1)\n\ndef normalize(trial):\n    mean = trial.mean(axis=-1, keepdims=True)\n    std = trial.std(axis=-1, keepdims=True) + 1e-6\n    return (trial - mean) / std","metadata":{"_uuid":"d7bd57f4-2583-4c4b-bf69-4223e4548fe7","_cell_guid":"be63ce12-0e74-4be6-a828-13ee303578b4","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-02-20T22:33:28.549398Z","iopub.execute_input":"2026-02-20T22:33:28.550107Z","iopub.status.idle":"2026-02-20T22:33:28.554954Z","shell.execute_reply.started":"2026-02-20T22:33:28.550080Z","shell.execute_reply":"2026-02-20T22:33:28.554260Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /kaggle/working/project/data/data_loader.py\nimport mne\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset\nfrom scipy.io import loadmat\nimport os\n\nfrom utils import normalize\n\nclass BCIV2aDataset(Dataset):\n    def __init__(self, gdf_path, mat_path=None):\n        print(f\"Loading {gdf_path}\")\n        raw = mne.io.read_raw_gdf(gdf_path, preload=True)\n\n        # Select only 22 EEG channels\n        eeg_channels = [ch for ch in raw.ch_names if 'EOG' not in ch.upper()]\n\n        if len(eeg_channels) != 22:\n            print(f\"Warning: Found {len(eeg_channels)} channels after EOG exclusion.\")\n            print(\"Available channels:\", raw.ch_names)\n            standard_names = [\n                'EEG-Fz', 'EEG-0', 'EEG-1', 'EEG-2', 'EEG-3', 'EEG-4',\n                'EEG-5', 'EEG-C3', 'EEG-6', 'EEG-C4', 'EEG-7',\n                'EEG-8', 'EEG-9', 'EEG-10', 'EEG-11', 'EEG-12',\n                'EEG-13', 'EEG-14', 'EEG-Pz', 'EEG-15', 'EEG-16', 'EEG-17'\n            ]\n            available = [ch for ch in standard_names if ch in raw.ch_names]\n            if len(available) == 22:\n                eeg_channels = available\n            else:\n                raise ValueError(f\"Could not select 22 EEG channels. Got {len(available)}\")\n\n        print(f\"Selected {len(eeg_channels)} EEG channels:\")\n        print(eeg_channels)\n\n        raw.pick_channels(eeg_channels, ordered=True)\n\n        raw.filter(8., 30., fir_design='firwin')\n\n        events, event_dict = mne.events_from_annotations(raw)\n\n        if mat_path is None:\n            # Training session\n            required_events = ['769', '770', '771', '772']\n            valid_event_id = {k: event_dict[k] for k in required_events if k in event_dict}\n\n            epochs = mne.Epochs(\n                raw, events, valid_event_id,\n                tmin=2.0, tmax=6.0, baseline=None, preload=True\n            )\n\n            data = epochs.get_data()\n            print(f\"Raw epochs shape (train): {data.shape}\")\n\n            data = np.stack([normalize(trial) for trial in data])\n\n            label_map = {\n                event_dict['769']: 0,\n                event_dict['770']: 1,\n                event_dict['771']: 2,\n                event_dict['772']: 3\n            }\n            labels = np.array([label_map[e] for e in epochs.events[:, -1]])\n\n        else:\n            # Evaluation session - FIXED ALIGNMENT\n            if '783' not in event_dict:\n                raise ValueError(\"Event 783 not found.\")\n\n            epochs = mne.Epochs(\n                raw, events, {'783': event_dict['783']},\n                tmin=2.0, tmax=6.0, baseline=None, preload=True\n            )\n\n            data = epochs.get_data()\n            print(f\"Raw epochs shape (eval): {data.shape}\")\n\n            mat = loadmat(mat_path)\n            all_labels = mat['classlabel'].squeeze() - 1\n\n            # Robust matching using event sample times\n            cue_id = event_dict['783']\n            all_cue_events = events[events[:, -1] == cue_id]\n\n            kept_sample_times = epochs.events[:, 0]\n            kept_indices = []\n\n            for sample_time in kept_sample_times:\n                match_idx = np.where(all_cue_events[:, 0] == sample_time)[0]\n                if len(match_idx) == 1:\n                    kept_indices.append(match_idx[0])\n                else:\n                    print(f\"Warning: No unique match for sample {sample_time}\")\n                    # Skip or handle â€” but for this dataset it should match\n\n            labels = all_labels[kept_indices]\n\n            print(f\"Aligned labels: kept {len(labels)} / {len(all_labels)} using event matching\")\n\n            if len(data) != len(labels):\n                raise AssertionError(\n                    f\"Mismatch after alignment: data={len(data)}, labels={len(labels)}\"\n                )\n\n        print(f\"Loaded {len(data)} trials\")\n\n        self.data = torch.tensor(data, dtype=torch.float32)\n        self.labels = torch.tensor(labels, dtype=torch.long)\n\n        assert len(self.data) == len(self.labels)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx], self.labels[idx]\n\n\ndef inspect_dataset(dataset, name):\n    print(f\"\\n========== {name} ==========\")\n    print(\"Total samples:\", len(dataset))\n    print(\"Data shape:\", dataset.data.shape)\n    print(\"Labels shape:\", dataset.labels.shape)\n    print(\"Unique labels:\", torch.unique(dataset.labels))\n    x, y = dataset[0]\n    print(\"\\nFirst sample shape:\", x.shape)\n    print(\"First sample label:\", y.item())\n\n\nif __name__ == \"__main__\":\n    DATA_DIR = \"/kaggle/input/datasets/bhaskarkumar01/bciciv-2a\"\n    subject = \"A01\"\n    train_gdf = os.path.join(DATA_DIR, f\"{subject}T.gdf\")\n    test_gdf = os.path.join(DATA_DIR, f\"{subject}E.gdf\")\n    test_mat = os.path.join(DATA_DIR, f\"{subject}E.mat\")\n\n    print(\"Loading datasets...\\n\")\n\n    train_dataset = BCIV2aDataset(train_gdf)\n    test_dataset = BCIV2aDataset(test_gdf, test_mat)\n\n    inspect_dataset(train_dataset, \"TRAIN DATASET\")\n    inspect_dataset(test_dataset, \"TEST DATASET\")\n    print(train_dataset)","metadata":{"_uuid":"a9baea0e-532a-497e-a5a9-ce9ce02fb5f8","_cell_guid":"678c53be-963b-40db-b269-c78fc67be83c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-02-20T22:33:35.040907Z","iopub.execute_input":"2026-02-20T22:33:35.041767Z","iopub.status.idle":"2026-02-20T22:33:35.049455Z","shell.execute_reply.started":"2026-02-20T22:33:35.041725Z","shell.execute_reply":"2026-02-20T22:33:35.048416Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /kaggle/working/project/data/data_loader.py\nimport mne\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset\nfrom scipy.io import loadmat\nimport os\nfrom utils import normalize\n\nclass BCIV2aDataset(Dataset):\n    def __init__(self, gdf_path, mat_path=None):\n        print(f\"Loading {gdf_path}\")\n        # Standardize MNE loading to avoid \"duplicate channel\" warnings\n        raw = mne.io.read_raw_gdf(gdf_path, preload=True, verbose=False)\n\n        # 1. Precise Channel Selection (Ensuring 22 EEG channels)\n        # BCI IV 2a has 22 EEG + 3 EOG. We strictly want the first 22.\n        eeg_channels = raw.ch_names[:22]\n        raw.pick_channels(eeg_channels)\n        \n        # 2. Filter 8-30Hz (The Motor Imagery Band)\n        raw.filter(8., 30., fir_design='firwin', skip_by_annotation='edge', verbose=False)\n\n        # 3. Handle Events\n        events, event_dict = mne.events_from_annotations(raw, verbose=False)\n\n        if mat_path is None:\n            # --- TRAINING SESSION ---\n            # Event IDs for classes: 769 (L), 770 (R), 771 (Foot), 772 (Tongue)\n            target_ids = [event_dict[k] for k in ['769', '770', '771', '772'] if k in event_dict]\n            \n            epochs = mne.Epochs(\n                raw, events, event_id=target_ids,\n                tmin=0.5, tmax=4.5, # 4-second window\n                baseline=None, preload=True, proj=False, verbose=False, on_missing='warning'\n            )\n            \n            data = epochs.get_data() # (N, 22, T)\n            # Map event codes back to 0-3\n            inv_dict = {v: k for k, v in event_dict.items()}\n            labels = np.array([int(inv_dict[e]) - 769 for e in epochs.events[:, -1]])\n            \n        else:\n            # --- EVALUATION SESSION ---\n            # For evaluation, we slice based on the \"783\" (Cue) marker\n            cue_id = event_dict['783']\n            \n            epochs = mne.Epochs(\n                raw, events, event_id=cue_id,\n                tmin=0.5, tmax=4.5,\n                baseline=None, preload=True, proj=False, verbose=False, on_missing='warning'\n            )\n            \n            data = epochs.get_data()\n            \n            # Load true labels from .mat\n            mat = loadmat(mat_path)\n            all_labels = mat['classlabel'].squeeze() - 1 # 1..4 -> 0..3\n            \n            # CRITICAL FIX: If MNE dropped trials due to 'bad' annotations, \n            # we must align the labels to the kept epochs.\n            # We use the event index as a guide.\n            kept_indices = []\n            all_cue_events = events[events[:, -1] == cue_id]\n            for event_time in epochs.events[:, 0]:\n                idx = np.where(all_cue_events[:, 0] == event_time)[0][0]\n                kept_indices.append(idx)\n            \n            labels = all_labels[kept_indices]\n\n        # 4. Final Processing\n        # Ensure we have exactly 1001 time points (standard for this dataset)\n        # If your tmin/tmax gives slightly different, we truncate/pad\n        if data.shape[-1] > 1000:\n            data = data[:, :, :1000]\n            \n        # Global Normalization (Z-score per trial)\n        data = np.stack([normalize(trial) for trial in data])\n\n        self.data = torch.tensor(data, dtype=torch.float32)\n        self.labels = torch.tensor(labels, dtype=torch.long)\n        \n        print(f\"Final Dataset: {self.data.shape[0]} trials, {self.data.shape[1]} channels\")\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx], self.labels[idx]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T22:49:10.260574Z","iopub.execute_input":"2026-02-20T22:49:10.260885Z","iopub.status.idle":"2026-02-20T22:49:10.268008Z","shell.execute_reply.started":"2026-02-20T22:49:10.260849Z","shell.execute_reply":"2026-02-20T22:49:10.267262Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/project/data/data_loader.py\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"# !python /kaggle/working/project/data/data_loader.py","metadata":{"_uuid":"fb257d8b-17d6-4ed2-8f87-3d31579f1dc9","_cell_guid":"9ebcceb7-dcb0-4d94-a26c-40c47edc7aaa","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-02-20T21:51:44.958453Z","iopub.execute_input":"2026-02-20T21:51:44.958953Z","iopub.status.idle":"2026-02-20T21:51:52.307386Z","shell.execute_reply.started":"2026-02-20T21:51:44.958919Z","shell.execute_reply":"2026-02-20T21:51:52.306714Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /kaggle/working/project/model/multiscale_cnn.py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nclass MultiScaleCNN(nn.Module):\n    def __init__(self, in_channels):\n        super(MultiScaleCNN, self).__init__()\n        # Change depthwise to full conv with 32 output filters (learn multiple spatial combinations)\n        self.spatial_conv = nn.Conv2d(\n            in_channels=1,\n            out_channels=32,\n            kernel_size=(in_channels, 1),  # (22, 1)\n            bias=False\n        )\n        self.spatial_bn = nn.BatchNorm2d(32)\n        # Rest remains the same\n        self.conv64 = nn.Conv2d(32, 64, kernel_size=(1, 64), padding=(0, 32))\n        self.conv32 = nn.Conv2d(32, 64, kernel_size=(1, 32), padding=(0, 16))\n        self.conv16 = nn.Conv2d(32, 64, kernel_size=(1, 16), padding=(0, 8))\n        self.bn = nn.BatchNorm2d(192)\n        self.dropout = nn.Dropout(0.5)\n        self.pool = nn.AvgPool2d(kernel_size=(1, 8))\n\n    def forward(self, x):\n        # x: (B, 1, C, T)\n        x = self.spatial_conv(x)  # Output: (B, 32, 1, T)\n        x = F.elu(self.spatial_bn(x))\n        # Multi-scale temporal (same as before)\n        f1 = F.elu(self.conv64(x))\n        f2 = F.elu(self.conv32(x))\n        f3 = F.elu(self.conv16(x))\n        out = torch.cat([f1, f2, f3], dim=1)  # (B, 192, 1, T)\n        out = self.bn(out)\n        out = self.dropout(out)\n        out = self.pool(out)  # (B, 192, 1, T/8)\n        return out","metadata":{"_uuid":"fd60f8c8-7470-4940-a7ef-d3ed3f6ba58b","_cell_guid":"f3fe80f7-0f96-4897-ad0c-cda9c4a0319f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-02-20T22:33:39.855690Z","iopub.execute_input":"2026-02-20T22:33:39.856266Z","iopub.status.idle":"2026-02-20T22:33:39.861253Z","shell.execute_reply.started":"2026-02-20T22:33:39.856236Z","shell.execute_reply":"2026-02-20T22:33:39.860597Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /kaggle/working/project/model/transformer.py\nimport torch\nimport torch.nn as nn\nimport math\n\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=1000):\n        super().__init__()\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1)\n        div_term = torch.exp(\n            torch.arange(0, d_model, 2) *\n            (-math.log(10000.0) / d_model)\n        )\n\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n\n        pe = pe.unsqueeze(0)\n        self.register_buffer(\"pe\", pe)\n\n    def forward(self, x):\n        return x + self.pe[:, :x.size(1)]\n\n\nclass TransformerEncoder(nn.Module):\n    def __init__(self, embed_dim):\n        super().__init__()\n\n        self.positional_encoding = PositionalEncoding(embed_dim)\n\n        layer = nn.TransformerEncoderLayer(\n            d_model=embed_dim,\n            nhead=2,\n            dim_feedforward=256,\n            dropout=0.1,\n            batch_first=True\n        )\n\n        self.encoder = nn.TransformerEncoder(layer, num_layers=2)\n\n    def forward(self, x):\n        x = self.positional_encoding(x)\n        return self.encoder(x)","metadata":{"_uuid":"12d89390-dca8-4997-aded-66827cd13779","_cell_guid":"b7045341-0ce7-47e6-a8d9-9abebe37eb01","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-02-20T22:33:41.882064Z","iopub.execute_input":"2026-02-20T22:33:41.882334Z","iopub.status.idle":"2026-02-20T22:33:41.887652Z","shell.execute_reply.started":"2026-02-20T22:33:41.882310Z","shell.execute_reply":"2026-02-20T22:33:41.886905Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /kaggle/working/project/model/cl_mst.py\nimport torch\nimport torch.nn as nn\nfrom model.multiscale_cnn import MultiScaleCNN\nfrom model.transformer import TransformerEncoder\n\nclass CL_MST(nn.Module):\n    def __init__(self, in_channels, num_classes):\n        super(CL_MST, self).__init__()\n\n        # Multi-scale CNN to extract temporal-spatial features\n        self.mscnn = MultiScaleCNN(in_channels)\n\n        # embed_dim matches the total concatenated channels from MSCNN (64 * 3 = 192)\n        self.transformer = TransformerEncoder(embed_dim=192)\n\n        # Classification Head\n        self.classifier = nn.Linear(192, num_classes)\n\n        # Projection Head for Supervised Contrastive Loss\n        self.projection_head = nn.Sequential(\n            nn.Linear(192, 128),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.Linear(128, 64)\n        )\n\n    def forward(self, x):\n        # Input shape: (Batch, Channels, Time) -> (B, 22, 1001)\n        \n        # 1. Expand for Conv2d: (B, 1, 22, 1001)\n        x = x.unsqueeze(1)\n\n        # 2. Extract features: (B, 192, 1, T_reduced)\n        feats = self.mscnn(x)\n        \n        # 3. Prepare for Transformer: Remove spatial dim and permute to (B, Seq_Len, Embedding)\n        # We use squeeze(2) instead of mean(dim=2) to be safer\n        feats = feats.squeeze(2)          # Shape: (B, 192, T_reduced)\n        feats = feats.permute(0, 2, 1)    # Shape: (B, T_reduced, 192)\n\n        # 4. Transformer Attention\n        encoded = self.transformer(feats)\n\n        # 5. Global Average Pooling over the time dimension\n        pooled = encoded.mean(dim=1)      # Shape: (B, 192)\n\n        # 6. Outputs\n        logits = self.classifier(pooled)\n        proj = self.projection_head(pooled)\n\n        return logits, proj","metadata":{"_uuid":"83240a63-2701-4f57-9182-06870c8fa33a","_cell_guid":"66e3381e-a280-4e95-b409-8ba346ea0542","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-02-20T22:33:48.677378Z","iopub.execute_input":"2026-02-20T22:33:48.677674Z","iopub.status.idle":"2026-02-20T22:33:48.683816Z","shell.execute_reply.started":"2026-02-20T22:33:48.677647Z","shell.execute_reply":"2026-02-20T22:33:48.683205Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /kaggle/working/project/loss.py\n# import torch.nn.functional as F\n\n# def contrastive_loss(z1, z2):\n#     z1 = F.normalize(z1, dim=1)\n#     z2 = F.normalize(z2, dim=1)\n#     return 1 - F.cosine_similarity(z1, z2).mean()\n\n\n\n# loss.py\n\nimport torch\nimport torch.nn.functional as F\n\ndef supervised_contrastive_loss(features, labels, temperature=0.07):\n    \"\"\"\n    features: (B, D)\n    labels: (B,)\n    \"\"\"\n\n    device = features.device\n    features = F.normalize(features, dim=1)\n\n    similarity_matrix = torch.matmul(features, features.T) / temperature\n\n    labels = labels.contiguous().view(-1, 1)\n    mask = torch.eq(labels, labels.T).float().to(device)\n\n    logits_mask = torch.ones_like(mask) - torch.eye(mask.shape[0]).to(device)\n    mask = mask * logits_mask\n\n    exp_sim = torch.exp(similarity_matrix) * logits_mask\n\n    log_prob = similarity_matrix - torch.log(exp_sim.sum(dim=1, keepdim=True) + 1e-8)\n\n    mean_log_prob_pos = (mask * log_prob).sum(dim=1) / (mask.sum(dim=1) + 1e-8)\n\n    loss = -mean_log_prob_pos.mean()\n\n    return loss","metadata":{"_uuid":"2ebebf65-f49b-4d2b-ad92-09a90175283c","_cell_guid":"b9e2c567-47f6-4e8a-89a2-16dd638b36cf","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-02-20T22:33:52.531625Z","iopub.execute_input":"2026-02-20T22:33:52.532170Z","iopub.status.idle":"2026-02-20T22:33:52.537138Z","shell.execute_reply.started":"2026-02-20T22:33:52.532141Z","shell.execute_reply":"2026-02-20T22:33:52.536253Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /kaggle/working/project/utils.py\nimport torch\nimport numpy as np\nfrom scipy.signal import butter, lfilter\n\ndef accuracy(preds, labels):\n    _, pred = torch.max(preds, 1)\n    return (pred == labels).float().mean().item()\n\ndef bandpass_filter(data, low=8, high=30, fs=250, order=4):\n    nyq = 0.5 * fs\n    low /= nyq\n    high /= nyq\n    b, a = butter(order, [low, high], btype=\"band\")\n    return lfilter(b, a, data, axis=-1)\n\ndef normalize(trial):\n    mean = trial.mean(axis=-1, keepdims=True)\n    std = trial.std(axis=-1, keepdims=True) + 1e-6\n    return (trial - mean) / std","metadata":{"_uuid":"495353c2-3122-4283-9ce6-b64e78e86846","_cell_guid":"5a0a252a-d4d7-4157-86a5-d3dc42cfba5d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-02-20T22:33:54.980611Z","iopub.execute_input":"2026-02-20T22:33:54.981298Z","iopub.status.idle":"2026-02-20T22:33:54.985770Z","shell.execute_reply.started":"2026-02-20T22:33:54.981268Z","shell.execute_reply":"2026-02-20T22:33:54.985106Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /kaggle/working/project/train.py\nimport os\nimport copy\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport random\n\nfrom torch.utils.data import DataLoader, Subset\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score, cohen_kappa_score\n\nfrom data.data_loader import BCIV2aDataset\nfrom model.cl_mst import CL_MST\nfrom loss import supervised_contrastive_loss\n\n# --- SETTINGS ---\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nDATA_DIR = \"/kaggle/input/datasets/bhaskarkumar01/bciciv-2a\"\nEPOCHS = 400  # Increased because augmentation requires more time to converge\nBATCH_SIZE = 64\nLR = 0.0007\nWEIGHT_DECAY = 0.05\n\n# --- AUGMENTATION FUNCTIONS ---\ndef apply_augmentation(x):\n    \"\"\"Applies random transformations to a batch of EEG data.\"\"\"\n    # 1. Add Gaussian Noise\n    if random.random() > 0.5:\n        noise = torch.randn_like(x) * 0.015\n        x = x + noise\n    \n    # 2. Time Shifting\n    if random.random() > 0.5:\n        shift = random.randint(-40, 40)\n        x = torch.roll(x, shifts=shift, dims=-1)\n        \n    # 3. Channel Dropout (Zeroing out a random channel)\n    if random.random() > 0.7:\n        ch_idx = random.randint(0, x.shape[1] - 1)\n        x[:, ch_idx, :] = 0\n        \n    return x\n\n# --- TRAINING STEP ---\ndef train_one_epoch(model, loader, criterion, optimizer, augment=True):\n    model.train()\n    total_loss = 0\n\n    for x, y in loader:\n        x, y = x.to(DEVICE), y.to(DEVICE)\n\n        if augment:\n            x = apply_augmentation(x)\n\n        optimizer.zero_grad()\n        logits, proj = model(x)\n\n        # Supervised Contrastive + Cross Entropy\n        ce_loss = criterion(logits, y)\n        con_loss = supervised_contrastive_loss(proj, y, temperature=0.2) # Softened temp\n\n        loss = ce_loss + 0.15 * con_loss\n        \n        loss.backward()\n        \n        # Prevent gradients from exploding in the Transformer\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        \n        optimizer.step()\n        total_loss += loss.item()\n\n    return total_loss / len(loader)\n\n# --- EVALUATION STEP ---\ndef evaluate(model, loader):\n    model.eval()\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(DEVICE)\n            logits, _ = model(x)\n            preds = torch.argmax(logits, dim=1).cpu().numpy()\n            all_preds.extend(preds)\n            all_labels.extend(y.numpy())\n\n    acc = accuracy_score(all_labels, all_preds)\n    kappa = cohen_kappa_score(all_labels, all_preds)\n    return acc, kappa\n\ndef main():\n    subject_results = []\n\n    # Loop through subjects\n    for i in range(1, 10):\n        subject = f\"A0{i}\"\n        print(f\"\\n{'='*30}\\n Subject {subject}\\n{'='*30}\")\n\n        train_gdf = os.path.join(DATA_DIR, f\"{subject}T.gdf\")\n        test_gdf = os.path.join(DATA_DIR, f\"{subject}E.gdf\")\n        test_mat = os.path.join(DATA_DIR, f\"{subject}E.mat\")\n\n        try:\n            train_dataset = BCIV2aDataset(train_gdf)\n            test_dataset = BCIV2aDataset(test_gdf, test_mat)\n        except Exception as e:\n            print(f\"Skipping {subject} due to error: {e}\")\n            continue\n\n        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n        fold_test_acc = []\n        \n        for fold, (train_idx, val_idx) in enumerate(kf.split(range(len(train_dataset)))):\n            print(f\"--- Fold {fold+1} ---\")\n\n            train_subset = Subset(train_dataset, train_idx)\n            val_subset = Subset(train_dataset, val_idx)\n\n            train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True)\n            val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False)\n            test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n            model = CL_MST(in_channels=22, num_classes=4).to(DEVICE)\n            criterion = nn.CrossEntropyLoss()\n            optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n            \n            # Scheduler helps the model settle after aggressive augmentation\n            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n\n            best_val_acc = 0\n            best_model_state = None\n\n            for epoch in range(EPOCHS):\n                train_loss = train_one_epoch(model, train_loader, criterion, optimizer, augment=True)\n                \n                # Validation check every 10 epochs to save time\n                if (epoch + 1) % 10 == 0 or epoch == 0:\n                    val_acc, _ = evaluate(model, val_loader)\n                    if val_acc > best_val_acc:\n                        best_val_acc = val_acc\n                        best_model_state = copy.deepcopy(model.state_dict())\n                \n                scheduler.step()\n\n            # Load best weights from this fold and test on Evaluation Session\n            model.load_state_dict(best_model_state)\n            test_acc, test_kappa = evaluate(model, test_loader)\n            \n            print(f\"Fold {fold+1} Test Acc: {test_acc*100:.2f}% | Kappa: {test_kappa:.4f}\")\n            fold_test_acc.append(test_acc)\n\n        subject_results.append(np.mean(fold_test_acc))\n        print(f\"Subject {subject} Average Accuracy: {subject_results[-1]*100:.2f}%\")\n\n    print(f\"\\n{'='*40}\\n FINAL OVERALL ACCURACY: {np.mean(subject_results)*100:.2f}%\\n{'='*40}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"8014222e-6b83-4409-8171-7916893c1fe8","_cell_guid":"09d2f30e-8a0d-4344-a986-b4386b30bcb8","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-02-20T22:34:30.934097Z","iopub.execute_input":"2026-02-20T22:34:30.934415Z","iopub.status.idle":"2026-02-20T22:34:30.941560Z","shell.execute_reply.started":"2026-02-20T22:34:30.934389Z","shell.execute_reply":"2026-02-20T22:34:30.940800Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python  /kaggle/working/project/train.py","metadata":{"_uuid":"1cf76b99-c872-4c89-94c7-354cd98e7171","_cell_guid":"df23ddbd-24c3-4e6a-8e4c-241b08d1c36d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2026-02-20T22:49:15.217001Z","iopub.execute_input":"2026-02-20T22:49:15.217387Z","iopub.status.idle":"2026-02-20T23:50:59.470686Z","shell.execute_reply.started":"2026-02-20T22:49:15.217361Z","shell.execute_reply":"2026-02-20T23:50:59.469870Z"}},"outputs":[{"name":"stdout","text":"\n==============================\n Subject A01\n==============================\nLoading /kaggle/input/datasets/bhaskarkumar01/bciciv-2a/A01T.gdf\n/usr/lib/python3.12/contextlib.py:144: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n  next(self.gen)\nNOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\nFinal Dataset: 288 trials, 22 channels\nLoading /kaggle/input/datasets/bhaskarkumar01/bciciv-2a/A01E.gdf\n/usr/lib/python3.12/contextlib.py:144: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n  next(self.gen)\nNOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\nFinal Dataset: 288 trials, 22 channels\n--- Fold 1 ---\nFold 1 Test Acc: 68.06% | Kappa: 0.5741\n--- Fold 2 ---\nFold 2 Test Acc: 66.32% | Kappa: 0.5509\n--- Fold 3 ---\nFold 3 Test Acc: 69.10% | Kappa: 0.5880\n--- Fold 4 ---\nFold 4 Test Acc: 67.71% | Kappa: 0.5694\n--- Fold 5 ---\nFold 5 Test Acc: 66.32% | Kappa: 0.5509\nSubject A01 Average Accuracy: 67.50%\n\n==============================\n Subject A02\n==============================\nLoading /kaggle/input/datasets/bhaskarkumar01/bciciv-2a/A02T.gdf\n/usr/lib/python3.12/contextlib.py:144: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n  next(self.gen)\nNOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\nFinal Dataset: 288 trials, 22 channels\nLoading /kaggle/input/datasets/bhaskarkumar01/bciciv-2a/A02E.gdf\n/usr/lib/python3.12/contextlib.py:144: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n  next(self.gen)\nNOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\nFinal Dataset: 288 trials, 22 channels\n--- Fold 1 ---\nFold 1 Test Acc: 42.71% | Kappa: 0.2361\n--- Fold 2 ---\nFold 2 Test Acc: 44.79% | Kappa: 0.2639\n--- Fold 3 ---\nFold 3 Test Acc: 41.32% | Kappa: 0.2176\n--- Fold 4 ---\nFold 4 Test Acc: 38.54% | Kappa: 0.1806\n--- Fold 5 ---\nFold 5 Test Acc: 41.32% | Kappa: 0.2176\nSubject A02 Average Accuracy: 41.74%\n\n==============================\n Subject A03\n==============================\nLoading /kaggle/input/datasets/bhaskarkumar01/bciciv-2a/A03T.gdf\n/usr/lib/python3.12/contextlib.py:144: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n  next(self.gen)\nNOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\nFinal Dataset: 288 trials, 22 channels\nLoading /kaggle/input/datasets/bhaskarkumar01/bciciv-2a/A03E.gdf\n/usr/lib/python3.12/contextlib.py:144: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n  next(self.gen)\nNOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\nFinal Dataset: 288 trials, 22 channels\n--- Fold 1 ---\nFold 1 Test Acc: 76.04% | Kappa: 0.6806\n--- Fold 2 ---\nFold 2 Test Acc: 82.64% | Kappa: 0.7685\n--- Fold 3 ---\nFold 3 Test Acc: 79.86% | Kappa: 0.7315\n--- Fold 4 ---\nFold 4 Test Acc: 71.88% | Kappa: 0.6250\n--- Fold 5 ---\nFold 5 Test Acc: 73.96% | Kappa: 0.6528\nSubject A03 Average Accuracy: 76.88%\n\n==============================\n Subject A04\n==============================\nLoading /kaggle/input/datasets/bhaskarkumar01/bciciv-2a/A04T.gdf\n/usr/lib/python3.12/contextlib.py:144: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n  next(self.gen)\nNOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\nFinal Dataset: 288 trials, 22 channels\nLoading /kaggle/input/datasets/bhaskarkumar01/bciciv-2a/A04E.gdf\n/usr/lib/python3.12/contextlib.py:144: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n  next(self.gen)\nNOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\nFinal Dataset: 288 trials, 22 channels\n--- Fold 1 ---\nFold 1 Test Acc: 48.96% | Kappa: 0.3194\n--- Fold 2 ---\nFold 2 Test Acc: 52.08% | Kappa: 0.3611\n--- Fold 3 ---\nFold 3 Test Acc: 54.86% | Kappa: 0.3981\n--- Fold 4 ---\nFold 4 Test Acc: 42.71% | Kappa: 0.2361\n--- Fold 5 ---\nFold 5 Test Acc: 49.31% | Kappa: 0.3241\nSubject A04 Average Accuracy: 49.58%\n\n==============================\n Subject A05\n==============================\nLoading /kaggle/input/datasets/bhaskarkumar01/bciciv-2a/A05T.gdf\n/usr/lib/python3.12/contextlib.py:144: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n  next(self.gen)\nNOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\nFinal Dataset: 288 trials, 22 channels\nLoading /kaggle/input/datasets/bhaskarkumar01/bciciv-2a/A05E.gdf\n/usr/lib/python3.12/contextlib.py:144: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n  next(self.gen)\nNOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\nFinal Dataset: 288 trials, 22 channels\n--- Fold 1 ---\nFold 1 Test Acc: 38.54% | Kappa: 0.1806\n--- Fold 2 ---\nFold 2 Test Acc: 33.33% | Kappa: 0.1111\n--- Fold 3 ---\nFold 3 Test Acc: 35.42% | Kappa: 0.1389\n--- Fold 4 ---\nFold 4 Test Acc: 37.50% | Kappa: 0.1667\n--- Fold 5 ---\nFold 5 Test Acc: 38.89% | Kappa: 0.1852\nSubject A05 Average Accuracy: 36.74%\n\n==============================\n Subject A06\n==============================\nLoading /kaggle/input/datasets/bhaskarkumar01/bciciv-2a/A06T.gdf\n/usr/lib/python3.12/contextlib.py:144: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n  next(self.gen)\nNOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\nFinal Dataset: 288 trials, 22 channels\nLoading /kaggle/input/datasets/bhaskarkumar01/bciciv-2a/A06E.gdf\n/usr/lib/python3.12/contextlib.py:144: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n  next(self.gen)\nNOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\nFinal Dataset: 288 trials, 22 channels\n--- Fold 1 ---\nFold 1 Test Acc: 41.67% | Kappa: 0.2222\n--- Fold 2 ---\nFold 2 Test Acc: 43.40% | Kappa: 0.2454\n--- Fold 3 ---\nFold 3 Test Acc: 44.44% | Kappa: 0.2593\n--- Fold 4 ---\nFold 4 Test Acc: 45.49% | Kappa: 0.2731\n--- Fold 5 ---\nFold 5 Test Acc: 41.32% | Kappa: 0.2176\nSubject A06 Average Accuracy: 43.26%\n\n==============================\n Subject A07\n==============================\nLoading /kaggle/input/datasets/bhaskarkumar01/bciciv-2a/A07T.gdf\n/usr/lib/python3.12/contextlib.py:144: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n  next(self.gen)\nNOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\nFinal Dataset: 288 trials, 22 channels\nLoading /kaggle/input/datasets/bhaskarkumar01/bciciv-2a/A07E.gdf\n/usr/lib/python3.12/contextlib.py:144: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n  next(self.gen)\nNOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\nFinal Dataset: 288 trials, 22 channels\n--- Fold 1 ---\nFold 1 Test Acc: 55.21% | Kappa: 0.4028\n--- Fold 2 ---\nFold 2 Test Acc: 53.47% | Kappa: 0.3796\n--- Fold 3 ---\nFold 3 Test Acc: 55.56% | Kappa: 0.4074\n--- Fold 4 ---\nFold 4 Test Acc: 63.54% | Kappa: 0.5139\n--- Fold 5 ---\nFold 5 Test Acc: 43.40% | Kappa: 0.2454\nSubject A07 Average Accuracy: 54.24%\n\n==============================\n Subject A08\n==============================\nLoading /kaggle/input/datasets/bhaskarkumar01/bciciv-2a/A08T.gdf\n/usr/lib/python3.12/contextlib.py:144: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n  next(self.gen)\nNOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\nFinal Dataset: 288 trials, 22 channels\nLoading /kaggle/input/datasets/bhaskarkumar01/bciciv-2a/A08E.gdf\n/usr/lib/python3.12/contextlib.py:144: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n  next(self.gen)\nNOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\nFinal Dataset: 288 trials, 22 channels\n--- Fold 1 ---\nFold 1 Test Acc: 74.65% | Kappa: 0.6620\n--- Fold 2 ---\nFold 2 Test Acc: 77.08% | Kappa: 0.6944\n--- Fold 3 ---\nFold 3 Test Acc: 75.69% | Kappa: 0.6759\n--- Fold 4 ---\nFold 4 Test Acc: 71.18% | Kappa: 0.6157\n--- Fold 5 ---\nFold 5 Test Acc: 71.88% | Kappa: 0.6250\nSubject A08 Average Accuracy: 74.10%\n\n==============================\n Subject A09\n==============================\nLoading /kaggle/input/datasets/bhaskarkumar01/bciciv-2a/A09T.gdf\n/usr/lib/python3.12/contextlib.py:144: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n  next(self.gen)\nNOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\nFinal Dataset: 288 trials, 22 channels\nLoading /kaggle/input/datasets/bhaskarkumar01/bciciv-2a/A09E.gdf\n/usr/lib/python3.12/contextlib.py:144: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n  next(self.gen)\nNOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\nFinal Dataset: 288 trials, 22 channels\n--- Fold 1 ---\nFold 1 Test Acc: 66.67% | Kappa: 0.5556\n--- Fold 2 ---\nFold 2 Test Acc: 69.10% | Kappa: 0.5880\n--- Fold 3 ---\nFold 3 Test Acc: 67.71% | Kappa: 0.5694\n--- Fold 4 ---\nFold 4 Test Acc: 67.71% | Kappa: 0.5694\n--- Fold 5 ---\nFold 5 Test Acc: 67.71% | Kappa: 0.5694\nSubject A09 Average Accuracy: 67.78%\n\n========================================\n FINAL OVERALL ACCURACY: 56.87%\n========================================\n\u001b[0m","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"","metadata":{"_uuid":"74b327b3-f18c-432f-902c-5697672e90ef","_cell_guid":"f3dd6387-c363-4616-82be-221a030cda36","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}